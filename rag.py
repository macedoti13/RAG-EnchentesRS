from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.prompts.chat import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain_core.documents.base import Document
from typing import List, Union, Optional, Dict
from langchain_openai import ChatOpenAI
from retriever import Retriever
from indexer import Indexer
from loader import Loader

class RAG:
    """
    A class for handling Retrieval-Augmented Generation (RAG) by combining document retrieval and language model generation.

    Attributes
    ----------
    completion_model : str
        The model to use for language generation.
    embedding_model : str
        The model to use for generating embeddings.
    db : Indexer
        The database indexer for document storage and retrieval.
    top_k : int
        The number of top documents to retrieve.
    retrieved_contexts : Dict[str, List[Document]]
        A dictionary storing the contexts retrieved for each query.

    Methods
    -------
    query(question: str) -> str
        Queries the RAG chain with the given question and returns the response.
    add_documents(urls: Union[str, List[str]]) -> None
        Adds documents from the specified URLs to the RAG.
    _add_documents_to_db(documents: List[Document]) -> None
        Adds documents to the database and updates the retriever.
    _create_retriever(db: Indexer, top_k: int = 10) -> Retriever
        Creates a retriever from the database.
    from_db(db_path: str, completion_model: Optional[str] = "gpt-3.5-turbo", embedding_model: Optional[str] = "text-embedding-3-small") -> "RAG"
        Creates a RAG instance from an existing database.
    _create_rag_chain() -> None
        Creates the RAG chain combining document retrieval and language generation.
    """

    def __init__(
            self, 
            completion_model: Optional[str] = "gpt-3.5-turbo", 
            embedding_model: Optional[str] = "text-embedding-3-small", 
            db: Optional[Indexer] = None, 
            top_k: Optional[int] = 10
        ) -> None:
        """
        Initializes the RAG instance with the specified models and database.

        Parameters
        ----------
        completion_model : Optional[str], optional
            The model to use for language generation (default is "gpt-3.5-turbo").
        embedding_model : Optional[str], optional
            The model to use for generating embeddings (default is "text-embedding-3-small").
        db : Optional[Indexer], optional
            The database indexer for document storage and retrieval (default is None).
        top_k : Optional[int], optional
            The number of top documents to retrieve (default is 10).
        """
        self.completion_model: str = completion_model
        self.embedding_model: str = embedding_model
        self.db: Indexer = db
        self.top_k = top_k
        self.retrieved_contexts: Dict[str, List[Document]] = {}

    def query(self, question: str) -> str:
        """
        Queries the RAG chain with the given question and returns the response.

        Parameters
        ----------
        question : str
            The question to query the RAG chain with.

        Returns
        -------
        str
            The response generated by the RAG chain.
        
        Raises
        ------
        ValueError
            If no documents have been added to the RAG.
        """
        if self.rag_chain is None:
            raise ValueError("No documents have been added to the RAG. Please add documents before querying.")
        
        response = self.rag_chain.invoke({"input": question})
        self.retrieved_contexts[question] = response["context"]
        return response["answer"]

    def add_documents(self, urls: Union[str, List[str]]) -> None:
        """
        Adds documents from the specified URLs to the RAG.

        Parameters
        ----------
        urls : Union[str, List[str]]
            The URLs to load documents from.
        """
        documents = Loader.load_documents(urls, chunk_model_name=self.completion_model)
        self._add_documents_to_db(documents)

    def _add_documents_to_db(self, documents: List[Document]) -> None:
        """
        Adds documents to the database and updates the retriever.

        Parameters
        ----------
        documents : List[Document]
            The documents to be added to the database.
        """
        if self.db is not None:
            Indexer.add_documents_to_db(documents, self.db)
        else:
            self.db = Indexer.create_new_db(documents, embedding_model=self.embedding_model, persist_directory="./db")
        
        self._create_retriever(self.db, top_k=self.top_k)
        self._create_rag_chain()

    def _create_retriever(self, db: Indexer, top_k: int = 10) -> Retriever:
        """
        Creates a retriever from the database.

        Parameters
        ----------
        db : Indexer
            The database indexer for document storage and retrieval.
        top_k : int, optional
            The number of top documents to retrieve (default is 10).

        Returns
        -------
        Retriever
            The created retriever.
        """
        self.retriever = Retriever.create_retriever_from_db(db, model=self.completion_model, top_k=top_k)

    @staticmethod
    def from_db(db_path: str, completion_model: Optional[str] = "gpt-3.5-turbo", embedding_model: Optional[str] = "text-embedding-3-small") -> "RAG":
        """
        Creates a RAG instance from an existing database.

        Parameters
        ----------
        db_path : str
            The path to the database.
        completion_model : Optional[str], optional
            The model to use for language generation (default is "gpt-3.5-turbo").
        embedding_model : Optional[str], optional
            The model to use for generating embeddings (default is "text-embedding-3-small").

        Returns
        -------
        RAG
            The created RAG instance.
        """
        db = Indexer.load_db(db_path, embedding_model=embedding_model)
        return RAG(db=db, completion_model=completion_model, embedding_model=embedding_model)

    def _create_rag_chain(self) -> None:
        """
        Creates the RAG chain combining document retrieval and language generation.
        """
        system_prompt: str = """
        Você é um assistente de perguntas e respostas inserido em uma RAG (Retrieval-Augmented Generation). \
        Você receberá um contexto extraído de documentos e uma pergunta feita pelo usuário. Sua tarefa é \
        responder à pergunta usando exclusivamente o contexto fornecido. Se o contexto não for fornecido ou \
        não for suficiente para responder à pergunta, responda com "Eu não sei". Utilize apenas as informações \
        contidas no contexto fornecido.

        Contexto fornecido: {context}
        """
        prompt = ChatPromptTemplate.from_messages([("system", system_prompt),("human", "{input}")])
        question_answer_chain = create_stuff_documents_chain(ChatOpenAI(model=self.completion_model), prompt)
        self.rag_chain = create_retrieval_chain(self.retriever, question_answer_chain)
